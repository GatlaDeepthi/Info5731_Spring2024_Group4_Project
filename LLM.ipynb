{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LLM:**"
      ],
      "metadata": {
        "id": "3ZVFwLaRW3I9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dvnoz_IWr7m"
      },
      "outputs": [],
      "source": [
        "#load reviews\n",
        "from typing import List\n",
        "from llama_index.core.readers import StringIterableReader\n",
        "from llama_index.core.schema import Document\n",
        "all_reviews.insert(0, \"Below are the top 5 reviews of \" + dropdown.value + \" product\")\n",
        "def load_context_data(context_data: List[str]) -> List[Document]:\n",
        "    ret: List[Document] = []\n",
        "    buff: str = \"\"\n",
        "    for text in context_data:\n",
        "        buff += text + \"\\n\\n\"\n",
        "        ret.append(buff)\n",
        "        buff = \"\"\n",
        "    return StringIterableReader().load_data(ret)\n",
        "tokens = load_context_data(all_reviews)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "\n",
        "# load mistralai LLm model\n",
        "mixtral_llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    token=huggingface_api_key\n",
        ")\n",
        "from llama_index.embeddings.jinaai import JinaEmbedding\n",
        "jina_embedding_model = JinaEmbedding(\n",
        "    api_key=Japi_key,\n",
        "    model=\"jina-embeddings-v2-base-en\",\n",
        ")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
        ")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=tokens, service_context=service_context\n",
        ")\n",
        "qa_prompt_tmpl = (\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"answer the query. Please be brief, concise, and complete.\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        ")\n",
        "\n",
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    service_context=service_context,\n",
        "    text_qa_template=qa_prompt,\n",
        "    response_mode=\"compact\",\n",
        ")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n"
      ],
      "metadata": {
        "id": "dHJljHBcWujj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = query_engine.query(\"text summary\")\n",
        "print(result.response)"
      ],
      "metadata": {
        "id": "w6bfY5hCWzeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}